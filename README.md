# **该仓库是对移动实时去噪网络MFDNet的非官方实现**

原文链接：[Lightweight network towards real-time image denoising on mobile devices](https://arxiv.org/abs/2211.04687)

## 1.Train

根据自己的数据集实际存放路径，修改train.py中的配置参数，然后运行train.py即可。

我用到的是"Train400"作为训练数据集，没有区分测试和验证集。若你有相应需求，可自行修改./utils/utils.py中的图像加载代码以及相应的train.py代码。

## 2.Test

同样，修改train.py当中的相应参数，随后运行test.py即可(训练和测试的配置参数代码并未分开编写)

## 3.注意事项

做完已经很久了，现在空了整理一下放上来。由于当时复现时，作者论文应该还没有中(至少我当时只能在arxiv上看到，还是预祝作者论文能够被顺利接收，哈哈😄)。所以论文中有一些细节点是连蒙带猜复现的，具体包括以下内容：

1：MFA模块的下采样不太清楚是用haar变换还是单纯的卷积做下采样，以及MFA模块的通道数是如何变化的(原文中只说了MFA的宽度是模型的1/4，但是具体是如何变化的没有提及)。此处我还是使用的haar变换。

2：根据haar变换增加的通道数以及原文中模型结构的通道数，作者应该是用三通道的彩色图像作为输入，但是在iphone11上用npu测试的时间也有14.2ms，考虑到个人需求，我用的是单通道作为输入，因此模型的通道数也做了相应的一个调整。

3：参数重置化部分，作者一笔带了过去，貌似没有给到详细的说明。此处我看报告里提到的ECB的参数重置化和作者仅差0.02db，因此用了ECB的参数重置化技术。

最后，haar变换的代码参考自[MWCNNv2](https://github.com/lpj-github-io/MWCNNv2/blob/master/MWCNN_code/model/common.py)

## 4.结论

跑出来，速度和效果都还可以。在cpu和3060 gpu上推理480*360的图像，时间分别是9ms和1ms，用ncnn在手机端推理，也能满足我个人的一个实时帧率推理。
